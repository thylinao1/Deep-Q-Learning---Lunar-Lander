{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c79539",
   "metadata": {},
   "source": [
    "Imports libraries and configures the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ada78",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a virtual display to render the Lunar Lander environment.\n",
    "Display(visible=0, size=(840, 480)).start();\n",
    "\n",
    "# Set the random seed for TensorFlow\n",
    "tf.random.set_seed(utils.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423be4f",
   "metadata": {},
   "source": [
    "Executes a step in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2409fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ad042",
   "metadata": {},
   "source": [
    "Executes a step in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddd407",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4702452",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b71ebb",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845a762c",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dcd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment and get the initial state.\n",
    "current_state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6301afa",
   "metadata": {},
   "source": [
    "Executes a step in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1469d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "# Display table with values.\n",
    "utils.display_table(current_state, action, next_state, reward, done)\n",
    "\n",
    "# Replace the `current_state` with the state after the action is taken\n",
    "current_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd222c",
   "metadata": {},
   "source": [
    "Builds and configures the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Q-Network\n",
    "q_network = Sequential([\n",
    "    Input(state_size),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(num_actions, activation = 'linear')    \n",
    "    ])\n",
    "\n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    Input(state_size),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(num_actions, activation = 'linear') \n",
    "    ])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = ALPHA)\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039e137",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c435cd",
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f01085b",
   "metadata": {},
   "source": [
    "Executes a step in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a742c8a2",
   "metadata": {},
   "source": [
    "Defines core computation or loss functions used by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88efb841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_loss\n",
    "\n",
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # Compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    \n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a).\n",
    "    y_targets = rewards + (gamma*max_qsa*(1-done_vals))\n",
    "    \n",
    "    # Get the q_values and reshape to match y_targets\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    # Compute the loss\n",
    "    \n",
    "    loss = MSE(y_targets, q_values) \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1cb1da",
   "metadata": {},
   "source": [
    "Executes a step in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6058c6",
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44764987",
   "metadata": {},
   "source": [
    "Defines core computation or loss functions used by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    utils.update_target_network(q_network, target_q_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6666c78d",
   "metadata": {},
   "source": [
    "Uses imported utility functions to streamline routine tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "    \n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    state = env.reset()\n",
    "    total_points = 0\n",
    "    \n",
    "    for t in range(max_num_timesteps):\n",
    "        \n",
    "        # From the current state S choose an action A using an ε-greedy policy\n",
    "        state_qn = np.expand_dims(state, axis=0)  # state needs to be the right shape for the q_network\n",
    "        q_values = q_network(state_qn)\n",
    "        action = utils.get_action(q_values, epsilon)\n",
    "        \n",
    "        # Take action A and receive reward R and the next state S'\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples (S,A,R,S') from D\n",
    "            experiences = utils.get_experiences(memory_buffer)\n",
    "            \n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "            agent_learn(experiences, GAMMA)\n",
    "        \n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    # Update the ε value\n",
    "    epsilon = utils.get_new_eps(epsilon)\n",
    "\n",
    "    # average of 200 points in the last 100 episodes.\n",
    "    if av_latest_points >= 200.0:\n",
    "        q_network.save('lunar_lander_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e60d7",
   "metadata": {},
   "source": [
    "Visualizes results for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the total point history along with the moving average\n",
    "utils.plot_history(total_point_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1a06e",
   "metadata": {},
   "source": [
    "Imports libraries and configures the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663cfa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings from imageio\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c3a71",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d32b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./videos/lunar_lander.mp4\"\n",
    "\n",
    "utils.create_video(filename, env, q_network)\n",
    "utils.embed_mp4(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
